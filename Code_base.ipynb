{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-RrUVFtXNdL"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "import sched\n",
        "\n",
        "base_urLs = \"https://pureportal.coventry.ac.uk/en/publications/\"\n",
        "\n",
        "import re\n",
        "\n",
        "def clean_query(input_query):\n",
        "    # Convert query to lowercase\n",
        "    input_query = input_query.lower()\n",
        "    \n",
        "    # Remove special characters and punctuation using regular expression\n",
        "    input_query = re.sub(r'[^\\w\\s]', '', input_query)\n",
        "    \n",
        "    # Replace spaces with \"+\"\n",
        "    input_query = input_query.replace(\" \", \"+\")\n",
        "    \n",
        "    return input_query.strip()\n",
        "\n",
        "\n",
        "# data extraction code idea gotten from https://machinelearningmastery.com/web-crawling-in-python/\n",
        "def extract_publication_data(publication):\n",
        "    titles = publication.select_one(\".title span\").text\n",
        "    authors = [author.text for author in publication.select(\".link.person span\")]\n",
        "    author = authors[0] if authors else \"\"\n",
        "    publiSher_element = publication.select_one(\".link[rel='Publisher'] span\")\n",
        "    publisher = publiSher_element.text.strip() if publiSher_element else \"\"\n",
        "    publIcation_status = publication.select_one(\".date\").text\n",
        "    publication_link = publication.select_one(\".title a\")[\"href\"]\n",
        "    cgl_author_Link = f\"https://pureportal.coventry.ac.uk/en/persons/{author.lower().replace(' ', '-')}\"\n",
        "    \n",
        "    return {\n",
        "        \"Title\": titles,\n",
        "        \"AuthOrs\": ', '.join(authors),\n",
        "        \"Publisher\": publisher,\n",
        "        \"Publication status\": publIcation_status,\n",
        "        \"Publication page Link\": publication_link,\n",
        "        \"CGL author Link\": cgl_author_Link\n",
        "    }\n",
        "\n",
        "def preprocess_publication_data(publications_list):\n",
        "    for publication in publications_list:\n",
        "        \n",
        "        # Capitalize the first letter of each word in the title\n",
        "        publication[\"Title\"] = publication[\"Title\"].title()\n",
        "        # Remove leading and trailing whitespaces from author names\n",
        "        authors = [author.strip() for author in publication[\"AuthOrs\"].split(\",\")]\n",
        "        publication[\"AuthOrs\"] = \", \".join(authors)\n",
        "\n",
        "        # Convert publisher names to lowercase\n",
        "        publication[\"Publisher\"] = publication[\"Publisher\"].lower()\n",
        "\n",
        "    return publications_list\n",
        "\n",
        "def rank_by_vector_space_model(query_from_user, publications_list):\n",
        "    # Create a list of publication titles and concatenate with the user query\n",
        "    titles = [publication[\"Title\"] for publication in publications_list]\n",
        "    documents = titles + [query_from_user]\n",
        "\n",
        "    # Use TF-IDF vectorizer\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "    # Calculate similarity between user query and publication titles\n",
        "    cosine_similarities = linear_kernel(tfidf_matrix[-1], tfidf_matrix[:-1]).flatten()\n",
        "\n",
        "    # Sort publications based on similarity score in descending order\n",
        "    ranked_publications = sorted(zip(publications_list, cosine_similarities), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    return [publication for publication, _ in ranked_publications]\n",
        "\n",
        "def crawl_publications(query_from_user, max_count=1):\n",
        "    query_from_user = clean_query(query_from_user)\n",
        "    crawled_page = 1\n",
        "    all_publications = []\n",
        "    search_urLs = f\"{base_urLs}?search={query_from_user}&page={crawled_page}\"\n",
        "\n",
        "    # Queue to store the URLs to crawl\n",
        "    queue = [search_urLs]\n",
        "    staff_publication_count = {}  # Dictionary to store the publication count per staff\n",
        "\n",
        "    while queue and crawled_page <= max_count:\n",
        "        urLs = queue.pop(0)\n",
        "        response = requests.get(urLs)\n",
        "        if response.status_code != 200:\n",
        "            print(\"Failed to retrieve data. Please try again later.\")\n",
        "            return all_publications\n",
        "\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "        publications = soup.select(\".result-container\")\n",
        "\n",
        "        if not publications:\n",
        "            break\n",
        "\n",
        "        for publication in publications:\n",
        "            publication_data = extract_publication_data(publication)\n",
        "            all_publications.append(publication_data)\n",
        "\n",
        "            # Count the number of publications per staff\n",
        "            author = publication_data['AuthOrs'].split(', ')[0]\n",
        "            staff_publication_count[author] = staff_publication_count.get(author, 0) + 1\n",
        "\n",
        "        crawled_page += 1\n",
        "        next_urLs = f\"{base_urLs}?search={query_from_user}&page={crawled_page}\"\n",
        "        queue.append(next_urLs)\n",
        "        time.sleep(2) #preserves the robots.txt rules\n",
        "\n",
        "    return all_publications, staff_publication_count\n",
        "\n",
        "# Get user input for the search query\n",
        "query_from_user = input(\" Enter your search query: \")\n",
        "\n",
        "publications_list, staff_publication_count = crawl_publications(query_from_user)\n",
        "\n",
        "if not publications_list:\n",
        "    print(\"No Publications Found.\")\n",
        "else:\n",
        "    # Apply preprocessing tasks to the crawled data\n",
        "    publications_list = preprocess_publication_data(publications_list)\n",
        "    \n",
        "    print(\"Number of Staff Whose Publications are Crawled:\", len(staff_publication_count))\n",
        "    print(\"Maximum Number Of Publications Per Staff:\", max(staff_publication_count.values()))\n",
        "    print(\"=\" * 40)\n",
        "    \n",
        "    # Rank publications using the Vector Space Model\n",
        "    ranked_publications = rank_by_vector_space_model(query_from_user, publications_list)\n",
        "    \n",
        "    for i, publication in enumerate(ranked_publications[:10]):\n",
        "        print(f\"Title: {publication['Title']}\")\n",
        "        print(f\"AuthOrs: {publication['AuthOrs']}\")\n",
        "        print(f\"Publisher: {publication['Publisher']}\")\n",
        "        print(f\"PubLication status: {publication['Publication status']}\")\n",
        "        print(f\"PubLication page Link: {publication['Publication page Link']}\")\n",
        "        print(f\"CGL author Link: {publication['CGL author Link']}\")\n",
        "        print(\"=\" * 40)\n",
        "        if i == 5:\n",
        "            break  # Stop after the first ten publications\n",
        "        \n",
        "    print(\"Number Of Staff Whose Publications Are Crawled:\", len(staff_publication_count))\n",
        "    print(\"Maximum Number Of Publications Per Staff:\", max(staff_publication_count.values()))\n",
        "       \n",
        "\n",
        "def crawl_and_display(query_from_user):\n",
        "    publications_list, staff_publication_count = crawl_publications(query_from_user)\n",
        "\n",
        "    if not publications_list:\n",
        "        print(\"No publications found.\")\n",
        "    else:\n",
        "        # Apply preprocessing tasks to the crawled data\n",
        "        publications_list = preprocess_publication_data(publications_list)\n",
        "\n",
        "        print(\"Number of staff whose publications are crawled:\", len(staff_publication_count))\n",
        "        print(\"Maximum number of publications per staff:\", max(staff_publication_count.values()))\n",
        "\n",
        "        # Rank publications using the Vector Space Model\n",
        "        ranked_publications = rank_by_vector_space_model(query_from_user, publications_list)\n",
        "\n",
        "        for i, publication in enumerate(ranked_publications[:10]):\n",
        "            print(f\"Title: {publication['Title']}\")\n",
        "            print(f\"AuthOrs: {publication['AuthOrs']}\")\n",
        "            print(f\"Publisher: {publication['Publisher']}\")\n",
        "            print(f\"Publication status: {publication['Publication status']}\")\n",
        "            print(f\"Publication page Link: {publication['Publication page Link']}\")\n",
        "            print(f\"CGL author Link: {publication['CGL author Link']}\")\n",
        "            print(\"-\" * 40)\n",
        "            if i == 9:\n",
        "                break  # Stop after the first ten publications\n",
        "\n",
        "        print(\"Number of staff whose publications are crawled:\", len(staff_publication_count))\n",
        "        print(\"Maximum Number Of Publications Per Staff:\", max(staff_publication_count.values()))\n",
        "\n",
        "def schedle_crawl():\n",
        "    query_from_user = input(\"Enter your search query: \")\n",
        "    crawl_and_display(query_from_user)\n",
        "    interval = 604800  # Schedule to crawl every 1 weekk (604,800 seconds)\n",
        "    schedler.enter(interval, 1, schedle_crawl)\n",
        "\n",
        "# Get user input for the search query\n",
        "query_from_user = input(\"Enter Your Search Query: \")\n",
        "\n",
        "# InitiaLize the scheduler\n",
        "schedler = sched.scheduler(time.time, time.sleep)\n",
        "schedler.enter(0, 1, schedle_crawl)\n",
        "\n",
        "# Run the scheduler\n",
        "schedler.run()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
